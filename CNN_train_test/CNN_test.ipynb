{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushsoni27/CNN_web/blob/master/CNN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBMjxoeF8oIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b076c1bf-bd68-49c6-ddd5-b7e71a4d8a98"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ-XTgDE8vgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fe21104-0f22-4688-b0f1-12e557118312"
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "\n",
        "os.chdir(\"drive/My Drive/Google colab projects/CNN_web\")\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\n\\nos.chdir(\"drive/My Drive/Google colab projects/CNN_web\")\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiZNchfU87BQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy\n",
        "import tensorflow\n",
        "\n",
        "def get_dataset_images(test_patch_path, im_dim=32, num_channels=3):\n",
        "    \"\"\"\n",
        "    Similar to the one used in training except that there is just a single testing binary file for testing the CIFAR10 trained models.\n",
        "    \"\"\"\n",
        "    print(\"Working on testing patch\")\n",
        "    data_dict = unpickle_patch(test_patch_path)\n",
        "    images_data = data_dict[b\"data\"]\n",
        "    dataset_array = numpy.reshape(images_data, newshape=(len(images_data), im_dim, im_dim, num_channels))\n",
        "    return dataset_array, data_dict[b\"labels\"]\n",
        "\n",
        "def unpickle_patch(file):\n",
        "    \"\"\"\n",
        "    Identical to the one used in training.\n",
        "    \"\"\"\n",
        "    patch_bin_file = open(file, 'rb')\n",
        "    patch_dict = pickle.load(patch_bin_file, encoding='bytes')\n",
        "    return patch_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtRjhA8B9JB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "91c27321-6972-4fb1-ddfb-c845c2cbf71c"
      },
      "source": [
        "#Dataset path containing the testing binary file to be decoded.\n",
        "patches_dir = \"cifar-10-batches-py/\"\n",
        "dataset_array, dataset_labels = get_dataset_images(test_patch_path=patches_dir + \"test_batch\", im_dim=32, num_channels=3)\n",
        "print(\"Size of data : \", dataset_array.shape)\n",
        "\n",
        "sess = tensorflow.Session()\n",
        "\n",
        "#Restoring the previously saved trained model.\n",
        "saved_model_path = 'model/'\n",
        "saver = tensorflow.train.import_meta_graph(saved_model_path+'model.ckpt.meta')\n",
        "saver.restore(sess=sess, save_path=saved_model_path+'model.ckpt')\n",
        "\n",
        "#Initalizing the varaibales.\n",
        "sess.run(tensorflow.global_variables_initializer())\n",
        "\n",
        "graph = tensorflow.get_default_graph()\n",
        "\n",
        "\"\"\"\n",
        "Restoring previous created tensors in the training phase based on their given tensor names in the training phase.\n",
        "Some of such tensors will be assigned the testing input data and their outcomes (data_tensor, label_tensor, and keep_prop).\n",
        "Others are helpful in assessing the model prediction accuracy (softmax_propabilities and softmax_predictions).\n",
        "\"\"\"\n",
        "softmax_propabilities = graph.get_tensor_by_name(name=\"softmax_probs:0\")\n",
        "softmax_predictions = tensorflow.argmax(softmax_propabilities, axis=1)\n",
        "data_tensor = graph.get_tensor_by_name(name=\"data_tensor:0\")\n",
        "label_tensor = graph.get_tensor_by_name(name=\"label_tensor:0\")\n",
        "keep_prop = graph.get_tensor_by_name(name=\"keep_prop:0\")\n",
        "\n",
        "#keep_prop is equal to 1 because there is no more interest to remove neurons in the testing phase.\n",
        "feed_dict_testing = {data_tensor: dataset_array,\n",
        "                     label_tensor: dataset_labels,\n",
        "                     keep_prop: 1.0}\n",
        "#Running the session to predict the outcomes of the testing samples.\n",
        "softmax_propabilities_, softmax_predictions_ = sess.run([softmax_propabilities, softmax_predictions],\n",
        "                                                      feed_dict=feed_dict_testing)\n",
        "#Assessing the model accuracy by counting number of correctly classified samples.\n",
        "correct = numpy.array(numpy.where(softmax_predictions_ == dataset_labels))\n",
        "correct = correct.size\n",
        "print(\"Correct predictions/10,000 : \", correct)\n",
        "\n",
        "#Closing the session\n",
        "sess.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on testing patch\n",
            "Size of data :  (10000, 32, 32, 3)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
            "Correct predictions/10,000 :  1002\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}