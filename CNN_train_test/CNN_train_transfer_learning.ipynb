{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_train_transfer_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushsoni27/CNN_web/blob/master/CNN_train_test/CNN_train_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu1KET_iJzjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ed3f85ba-62a1-437a-9525-3e86f014a5dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxx1lsSeJ-j3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "708f1f39-c8ba-4d8a-f8d0-0a1f1afb7a02"
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "\n",
        "os.chdir(\"drive/My Drive/Google colab projects/CNN_web\")\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\n\\nos.chdir(\"drive/My Drive/Google colab projects/CNN_web\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzY6zO01KLIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patches_dir = \"cifar-10-batches-py/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM8Ltge-KkSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFn_hAE2MvhZ",
        "colab_type": "text"
      },
      "source": [
        "##Download and load pre-trained *Inception V3* model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj_oFMgfKy59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN7tfb46MOxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "26dc812c-8c12-4a90-c352-0c7440b57126"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pre_trained_model = InceptionV3(\n",
        "    input_shape=(32, 32, 3), include_top=False, weights=None)\n",
        "pre_trained_model.load_weights(local_weights_file)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-508cc644624f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlocal_weights_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m pre_trained_model = InceptionV3(\n\u001b[0;32m----> 5\u001b[0;31m     input_shape=(32, 32, 3), include_top=False, weights=None)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpre_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_weights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_applications/inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         weights=weights)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    320\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'x'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                                      \u001b[0;34m'; got `input_shape='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                                      str(input_shape) + '`')\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input size must be at least 75x75; got `input_shape=(32, 32, 3)`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGekmgtuMiwS",
        "colab_type": "text"
      },
      "source": [
        "##Functions for loading data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR_OyPBqMVBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle_patch(file):\n",
        "    import pickle\n",
        "    \n",
        "    with open(file, 'rb') as fo:\n",
        "        dicti = pickle.load(fo, encoding='bytes')\n",
        "    return dicti\n",
        "\n",
        "def get_patch(data, labels, percent=70):\n",
        "    \"\"\"\n",
        "    Returning patch to train the CNN.\n",
        "    :param data: Complete input data after being encoded and reshaped.\n",
        "    :param labels: Labels of the entire dataset.\n",
        "    :param percent: Percent of samples to get returned in each patch.\n",
        "    :return: Subset of the data (patch) to train the CNN model.\n",
        "    \"\"\"\n",
        "    #Using the percent of samples per patch to return the actual number of samples to get returned.\n",
        "    num_elements = np.uint32(percent*data.shape[0]/100)\n",
        "    shuffled_labels = labels#Temporary variable to hold the data after being shuffled.\n",
        "    np.random.shuffle(shuffled_labels)#Randomly reordering the labels.\n",
        "    \"\"\"\n",
        "    The previously specified percent of the data is returned starting from the beginning until meeting the required number of samples. \n",
        "    The labels indices are also used to return their corresponding input images samples.\n",
        "    \"\"\"\n",
        "    return data[shuffled_labels[:num_elements], :, :, :], shuffled_labels[:num_elements]\n",
        "\n",
        "def get_dataset_images(dataset_path, im_dim=32, num_channels=3):\n",
        "\n",
        "    \"\"\"\n",
        "    This function accepts the dataset path, reads the data, and returns it after being reshaped to match the requierments of the CNN.\n",
        "    :param dataset_path:Path of the CIFAR10 dataset binary files.\n",
        "    :param im_dim:Number of rows and columns in each image. The image is expected to be rectangular.\n",
        "    :param num_channels:Number of color channels in the image.\n",
        "    :return:Returns the input data after being reshaped and output labels.\n",
        "    \"\"\"\n",
        "    num_files = 5                 #Number of training binary files in the CIFAR10 dataset.\n",
        "    images_per_file = 10000     #Number of samples withing each binary file.\n",
        "    files_names = os.listdir(patches_dir)   #Listing the binary files in the dataset path.\n",
        "    \"\"\"\n",
        "    Creating an empty array to hold the entire training data after being reshaped.\n",
        "    The dataset has 5 binary files holding the data. Each binary file has 10,000 samples. Total number of samples in the dataset is 5*10,000=50,000.\n",
        "    Each sample has a total of 3,072 pixels. These pixels are reshaped to form a RGB image of shape 32x32x3.\n",
        "    Finally, the entire dataset has 50,000 samples and each sample of shape 32x32x3 (50,000x32x32x3).\n",
        "    \"\"\"\n",
        "    dataset_array = np.zeros(shape=(num_files * images_per_file, im_dim, im_dim, num_channels), dtype=np.uint8)\n",
        "    #Creating an empty array to hold the labels of each input sample. Its size is 50,000 to hold the label of each sample in the dataset.\n",
        "    dataset_labels = np.zeros(shape=(num_files * images_per_file), dtype=np.uint8)\n",
        "    index = 0#Index variable to count number of training binary files being processed.\n",
        "    for file_name in files_names:\n",
        "        \"\"\"\n",
        "        Because the CIFAR10 directory does not only contain the desired training files and has some  other files, it is required to filter the required files.\n",
        "        Training files start by 'data_batch_' which is used to test whether the file is for training or not.\n",
        "        \"\"\"\n",
        "        if file_name[0:len(file_name) - 1] == \"data_batch_\":\n",
        "            print(\"Working on : \", file_name)\n",
        "            \"\"\"\n",
        "            Appending the path of the binary files to the name of the current file.\n",
        "            Then the complete path of the binary file is used to decoded the file and return the actual pixels values.\n",
        "            \"\"\"\n",
        "            data_dict = unpickle_patch(dataset_path+file_name)\n",
        "            \"\"\"\n",
        "            Returning the data using its key 'data' in the dictionary.\n",
        "            Character b is used before the key to tell it is binary string.\n",
        "            \"\"\"\n",
        "            images_data = data_dict[b\"data\"]\n",
        "            #Reshaping all samples in the current binary file to be of 32x32x3 shape.\n",
        "            images_data_reshaped = np.reshape(images_data, newshape=(len(images_data), im_dim, im_dim, num_channels))\n",
        "            #Appending the data of the current file after being reshaped.\n",
        "            dataset_array[index * images_per_file:(index + 1) * images_per_file, :, :, :] = images_data_reshaped\n",
        "            #Appening the labels of the current file.\n",
        "            dataset_labels[index * images_per_file:(index + 1) * images_per_file] = data_dict[b\"labels\"]\n",
        "            index = index + 1#Incrementing the counter of the processed training files by 1 to accept new file.\n",
        "    return dataset_array, dataset_labels#Returning the training input data and output labels.\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}