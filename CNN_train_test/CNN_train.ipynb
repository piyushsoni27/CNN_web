{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushsoni27/CNN_web/blob/master/CNN_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG6XE327ziGA",
        "colab_type": "text"
      },
      "source": [
        "##Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYI7OWc5ytzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ca8e5e79-89a2-4635-d9c6-bf34265086d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jRvWkGtzpH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e213baab-a5dd-4b7b-8165-3b198a6d9374"
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "\n",
        "os.chdir(\"drive/My Drive/Google colab projects/CNN_web\")\n",
        "\"\"\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\n\\nos.chdir(\"drive/My Drive/Google colab projects/CNN_web\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ivcqxwr0CEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patches_dir = \"cifar-10-batches-py/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDrosd7WwptY",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMgP8tPpwpLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE-OKPYSw1lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle_patch(file):\n",
        "    import pickle\n",
        "    \n",
        "    with open(file, 'rb') as fo:\n",
        "        dicti = pickle.load(fo, encoding='bytes')\n",
        "    return dicti\n",
        "\n",
        "def get_patch(data, labels, percent=70):\n",
        "    \"\"\"\n",
        "    Returning patch to train the CNN.\n",
        "    :param data: Complete input data after being encoded and reshaped.\n",
        "    :param labels: Labels of the entire dataset.\n",
        "    :param percent: Percent of samples to get returned in each patch.\n",
        "    :return: Subset of the data (patch) to train the CNN model.\n",
        "    \"\"\"\n",
        "    #Using the percent of samples per patch to return the actual number of samples to get returned.\n",
        "    num_elements = np.uint32(percent*data.shape[0]/100)\n",
        "    shuffled_labels = labels#Temporary variable to hold the data after being shuffled.\n",
        "    np.random.shuffle(shuffled_labels)#Randomly reordering the labels.\n",
        "    \"\"\"\n",
        "    The previously specified percent of the data is returned starting from the beginning until meeting the required number of samples. \n",
        "    The labels indices are also used to return their corresponding input images samples.\n",
        "    \"\"\"\n",
        "    return data[shuffled_labels[:num_elements], :, :, :], shuffled_labels[:num_elements]\n",
        "\n",
        "def get_dataset_images(dataset_path, im_dim=32, num_channels=3):\n",
        "\n",
        "    \"\"\"\n",
        "    This function accepts the dataset path, reads the data, and returns it after being reshaped to match the requierments of the CNN.\n",
        "    :param dataset_path:Path of the CIFAR10 dataset binary files.\n",
        "    :param im_dim:Number of rows and columns in each image. The image is expected to be rectangular.\n",
        "    :param num_channels:Number of color channels in the image.\n",
        "    :return:Returns the input data after being reshaped and output labels.\n",
        "    \"\"\"\n",
        "    num_files = 5                 #Number of training binary files in the CIFAR10 dataset.\n",
        "    images_per_file = 10000     #Number of samples withing each binary file.\n",
        "    files_names = os.listdir(patches_dir)   #Listing the binary files in the dataset path.\n",
        "    \"\"\"\n",
        "    Creating an empty array to hold the entire training data after being reshaped.\n",
        "    The dataset has 5 binary files holding the data. Each binary file has 10,000 samples. Total number of samples in the dataset is 5*10,000=50,000.\n",
        "    Each sample has a total of 3,072 pixels. These pixels are reshaped to form a RGB image of shape 32x32x3.\n",
        "    Finally, the entire dataset has 50,000 samples and each sample of shape 32x32x3 (50,000x32x32x3).\n",
        "    \"\"\"\n",
        "    dataset_array = np.zeros(shape=(num_files * images_per_file, im_dim, im_dim, num_channels), dtype=np.uint8)\n",
        "    #Creating an empty array to hold the labels of each input sample. Its size is 50,000 to hold the label of each sample in the dataset.\n",
        "    dataset_labels = np.zeros(shape=(num_files * images_per_file), dtype=np.uint8)\n",
        "    index = 0#Index variable to count number of training binary files being processed.\n",
        "    for file_name in files_names:\n",
        "        \"\"\"\n",
        "        Because the CIFAR10 directory does not only contain the desired training files and has some  other files, it is required to filter the required files.\n",
        "        Training files start by 'data_batch_' which is used to test whether the file is for training or not.\n",
        "        \"\"\"\n",
        "        if file_name[0:len(file_name) - 1] == \"data_batch_\":\n",
        "            print(\"Working on : \", file_name)\n",
        "            \"\"\"\n",
        "            Appending the path of the binary files to the name of the current file.\n",
        "            Then the complete path of the binary file is used to decoded the file and return the actual pixels values.\n",
        "            \"\"\"\n",
        "            data_dict = unpickle_patch(dataset_path+file_name)\n",
        "            \"\"\"\n",
        "            Returning the data using its key 'data' in the dictionary.\n",
        "            Character b is used before the key to tell it is binary string.\n",
        "            \"\"\"\n",
        "            images_data = data_dict[b\"data\"]\n",
        "            #Reshaping all samples in the current binary file to be of 32x32x3 shape.\n",
        "            images_data_reshaped = np.reshape(images_data, newshape=(len(images_data), im_dim, im_dim, num_channels))\n",
        "            #Appending the data of the current file after being reshaped.\n",
        "            dataset_array[index * images_per_file:(index + 1) * images_per_file, :, :, :] = images_data_reshaped\n",
        "            #Appening the labels of the current file.\n",
        "            dataset_labels[index * images_per_file:(index + 1) * images_per_file] = data_dict[b\"labels\"]\n",
        "            index = index + 1#Incrementing the counter of the processed training files by 1 to accept new file.\n",
        "    return dataset_array, dataset_labels#Returning the training input data and output labels.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OglH7ojlw8sF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_conv_layer(input_data, filter_size, num_filters):\n",
        "\n",
        "    \"\"\"\n",
        "    Builds the CNN convolution (conv) layer.\n",
        "    :param input_data:patch data to be processed.\n",
        "    :param filter_size:#Number of rows and columns of each filter. It is expected to have a rectangular filter.\n",
        "    :param num_filters:Number of filters.\n",
        "    :return:The last fully connected layer of the network.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Preparing the filters of the conv layer by specifiying its shape. \n",
        "    Number of channels in both input image and each filter must match.\n",
        "    Because number of channels is specified in the shape of the input image as the last value, index of -1 works fine.\n",
        "    \"\"\"\n",
        "    filters = tf.Variable(tf.truncated_normal(shape=(filter_size, filter_size, tf.cast(input_data.shape[-1], dtype=tf.int32), num_filters),\n",
        "                                                              stddev=0.05))\n",
        "    print(\"Size of conv filters bank : \", filters.shape)\n",
        "\n",
        "    \"\"\"\n",
        "    Building the convolution layer by specifying the input data, filters, strides along each of the 4 dimensions, and the padding.\n",
        "    Padding value of 'VALID' means the some borders of the input image will be lost in the result based on the filter size.\n",
        "    \"\"\"\n",
        "    conv_layer = tf.nn.conv2d(input=input_data,\n",
        "                                      filter=filters,\n",
        "                                      strides=[1, 1, 1, 1],\n",
        "                                      padding=\"VALID\")\n",
        "    print(\"Size of conv result : \", conv_layer.shape)\n",
        "    return filters, conv_layer#Returing the filters and the convolution layer result.\n",
        "  \n",
        "def dropout_flatten_layer(previous_layer, keep_prop):\n",
        "    \"\"\"\n",
        "    Applying the dropout layer.\n",
        "    :param previous_layer: Result of the previous layer to the dropout layer.\n",
        "    :param keep_prop: Probability of keeping neurons.\n",
        "    :return: flattened array.\n",
        "    \"\"\"\n",
        "    dropout = tf.nn.dropout(x=previous_layer, keep_prob=keep_prop)\n",
        "    num_features = dropout.get_shape()[1:].num_elements()\n",
        "    layer = tf.reshape(previous_layer, shape=(-1, num_features))#Flattening the results.\n",
        "    return layer\n",
        "\n",
        "def fc_layer(flattened_layer, num_inputs, num_outputs):\n",
        "    \"\"\"\n",
        "    uilds a fully connected (FC) layer.\n",
        "    :param flattened_layer: Previous layer after being flattened.\n",
        "    :param num_inputs: Number of inputs in the previous layer.\n",
        "    :param num_outputs: Number of outputs to be returned in such FC layer.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    #Preparing the set of weights for the FC layer. It depends on the number of inputs and number of outputs.\n",
        "    fc_weights = tf.Variable(tf.truncated_normal(shape=(num_inputs, num_outputs),\n",
        "                                                              stddev=0.05))\n",
        "    #Matrix multiplication between the flattened array and the set of weights.\n",
        "    fc_resultl = tf.matmul(flattened_layer, fc_weights)\n",
        "    return fc_resultl#Output of the FC layer (result of matrix multiplication).\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4RKlNkTxJFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_CNN(input_data, num_classes, keep_prop):\n",
        "\n",
        "    \"\"\"\n",
        "    Builds the CNN architecture by stacking conv, relu, pool, dropout, and fully connected layers.\n",
        "    :param input_data:patch data to be processed.\n",
        "    :param num_classes:Number of classes in the dataset. It helps determining the number of outputs in the last fully connected layer.\n",
        "    :param keep_prop:probability of dropping neurons in the dropout layer.\n",
        "    :return: last fully connected layer.\n",
        "    \"\"\"\n",
        "    #Preparing the first convolution layer.\n",
        "    filters1, conv_layer1 = create_conv_layer(input_data=input_data, filter_size=5, num_filters=4)\n",
        "    \"\"\"\n",
        "    Applying ReLU activation function over the conv layer output. \n",
        "    It returns a new array of the same shape as the input array.\n",
        "    \"\"\"\n",
        "    relu_layer1 = tf.nn.relu(conv_layer1)\n",
        "    print(\"Size of relu1 result : \", relu_layer1.shape)\n",
        "    \"\"\"\n",
        "    Max pooling is applied to the ReLU layer result to achieve translation invariance.\n",
        "    It returns a new array of a different shape from the the input array relative to the strides and kernel size used.\n",
        "    \"\"\"\n",
        "    max_pooling_layer1 = tf.nn.max_pool(value=relu_layer1,\n",
        "                                                ksize=[1, 2, 2, 1],\n",
        "                                                strides=[1, 1, 1, 1],\n",
        "                                                padding=\"VALID\")\n",
        "    print(\"Size of maxpool1 result : \", max_pooling_layer1.shape)\n",
        "\n",
        "    #Similar to the previous conv-relu-pool layers, new layers are just stacked to complete the CNN architecture.\n",
        "    #Conv layer with 3 filters and each filter is of sisze of 5x5.\n",
        "    filters2, conv_layer2 = create_conv_layer(input_data=max_pooling_layer1, filter_size=7, num_filters=3)\n",
        "    relu_layer2 = tf.nn.relu(conv_layer2)\n",
        "    print(\"Size of relu2 result : \", relu_layer2.shape)\n",
        "    max_pooling_layer2 = tf.nn.max_pool(value=relu_layer2,\n",
        "                                                ksize=[1, 2, 2, 1],\n",
        "                                                strides=[1, 1, 1, 1],\n",
        "                                                padding=\"VALID\")\n",
        "    print(\"Size of maxpool2 result : \", max_pooling_layer2.shape)\n",
        "\n",
        "    #Conv layer with 2 filters and a filter sisze of 5x5.\n",
        "    filters3, conv_layer3 = create_conv_layer(input_data=max_pooling_layer2, filter_size=5, num_filters=2)\n",
        "    relu_layer3 = tf.nn.relu(conv_layer3)\n",
        "    print(\"Size of relu3 result : \", relu_layer3.shape)\n",
        "    max_pooling_layer3 = tf.nn.max_pool(value=relu_layer3,\n",
        "                                                ksize=[1, 2, 2, 1],\n",
        "                                                strides=[1, 1, 1, 1],\n",
        "                                                padding=\"VALID\")\n",
        "    print(\"Size of maxpool3 result : \", max_pooling_layer3.shape)\n",
        "\n",
        "    #Adding dropout layer before the fully connected layers to avoid overfitting.\n",
        "    flattened_layer = dropout_flatten_layer(previous_layer=max_pooling_layer3, keep_prop=keep_prop)\n",
        "\n",
        "    #First fully connected (FC) layer. It accepts the result of the dropout layer after being flattened (1D).\n",
        "    fc_resultl = fc_layer(flattened_layer=flattened_layer, num_inputs=flattened_layer.get_shape()[1:].num_elements(),\n",
        "                          num_outputs=200)\n",
        "    #Second fully connected layer accepting the output of the previous fully connected layer. Number of outputs is equal to the number of dataset classes.\n",
        "    fc_result2 = fc_layer(flattened_layer=fc_resultl, num_inputs=fc_resultl.get_shape()[1:].num_elements(),\n",
        "                          num_outputs=num_classes)\n",
        "    print(\"Fully connected layer results : \", fc_result2)\n",
        "    return fc_result2#Returning the result of the last FC layer.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEVxnqQ2xERw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "632edb51-4c6e-4dd1-85e1-bd08299cf9a7"
      },
      "source": [
        "\n",
        "#Nnumber of classes in the dataset. Used to specify number of outputs in the last fully connected layer.\n",
        "num_datatset_classes = 10\n",
        "#Number of rows & columns in each input image. The image is expected to be rectangular Used to reshape the images and specify the input tensor shape.\n",
        "im_dim = 32\n",
        "#Number of channels in rach input image. Used to reshape the images and specify the input tensor shape.\n",
        "num_channels = 3\n",
        "\n",
        "\n",
        "dataset_array, dataset_labels = get_dataset_images(dataset_path=patches_dir, im_dim=im_dim, num_channels=num_channels)\n",
        "print(\"Size of data : \", dataset_array.shape)\n",
        "\n",
        "\"\"\"\n",
        "Input tensor to hold the data read above. It is the entry point of the computational graph.\n",
        "The given name of 'data_tensor' is useful for retreiving it when restoring the trained model graph for testing.\n",
        "\"\"\"\n",
        "data_tensor = tf.placeholder(tf.float32, shape=[None, im_dim, im_dim, num_channels], name='data_tensor')\n",
        "\"\"\"\n",
        "Tensor to hold the outputs label. \n",
        "The name \"label_tensor\" is used for accessing the tensor when tesing the saved trained model after being restored.\n",
        "\"\"\"\n",
        "label_tensor = tf.placeholder(tf.float32, shape=[None], name='label_tensor')\n",
        "\n",
        "#The probability of dropping neurons in the dropout layer. It is given a name for accessing it later.\n",
        "keep_prop = tf.Variable(initial_value=0.5, name=\"keep_prop\")\n",
        "\n",
        "#Building the CNN architecure and returning the last layer which is the fully connected layer.\n",
        "fc_result2 = create_CNN(input_data=data_tensor, num_classes=num_datatset_classes, keep_prop=keep_prop)\n",
        "\n",
        "\"\"\"\n",
        "Predicitions probabilities of the CNN for each training sample.\n",
        "Each sample has a probability for each of the 10 classes in the dataset.\n",
        "Such tensor is given a name for accessing it later.\n",
        "\"\"\"\n",
        "softmax_propabilities = tf.nn.softmax(fc_result2, name=\"softmax_probs\")\n",
        "\n",
        "\"\"\"\n",
        "Predicitions labels of the CNN for each training sample.\n",
        "The input sample is classified as the class of the highest probability.\n",
        "axis=1 indicates that maximum of values in the second axis is to be returned. This returns that maximum class probability fo each sample.\n",
        "\"\"\"\n",
        "softmax_predictions = tf.argmax(softmax_propabilities, axis=1)\n",
        "\n",
        "#Cross entropy of the CNN based on its calculated probabilities.\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=tf.reduce_max(input_tensor=softmax_propabilities, reduction_indices=[1]),\n",
        "                                                                labels=label_tensor)\n",
        "#Summarizing the cross entropy into a single value (cost) to be minimized by the learning algorithm.\n",
        "cost = tf.reduce_mean(cross_entropy)\n",
        "#Minimizng the network cost using the Gradient Descent optimizer with a learning rate is 0.01.\n",
        "ops = tf.train.GradientDescentOptimizer(learning_rate=.01).minimize(cost)\n",
        "\n",
        "#Creating a new TensorFlow Session to process the computational graph.\n",
        "sess = tf.Session()\n",
        "#Wiriting summary of the graph to visualize it using TensorBoard.\n",
        "tf.summary.FileWriter(logdir=\"./log/\", graph=sess.graph)\n",
        "#Initializing the variables of the graph.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\"\"\"\n",
        "Because it may be impossible to feed the complete data to the CNN on normal machines, it is recommended to split the data into a number of patches.\n",
        "A percent of traning samples is used to create each path. Samples for each path can be randomly selected.\n",
        "\"\"\"\n",
        "num_patches = 5#Number of patches\n",
        "for patch_num in np.arange(num_patches):\n",
        "    print(\"Patch : \", str(patch_num))\n",
        "    percent = 80 #percent of samples to be included in each path.\n",
        "    #Getting the input-output data of the current path.\n",
        "    shuffled_data, shuffled_labels = get_patch(data=dataset_array, labels=dataset_labels, percent=percent)\n",
        "    #Data required for cnn operation. 1)Input Images, 2)Output Labels, and 3)Dropout probability\n",
        "    cnn_feed_dict = {data_tensor: shuffled_data,\n",
        "                     label_tensor: shuffled_labels,\n",
        "                     keep_prop: 0.5}\n",
        "    \"\"\"\n",
        "    Training the CNN based on the current patch. \n",
        "    CNN error is used as input in the run to minimize it.\n",
        "    SoftMax predictions are returned to compute the classification accuracy.\n",
        "    \"\"\"\n",
        "    softmax_predictions_, _ = sess.run([softmax_predictions, ops], feed_dict=cnn_feed_dict)\n",
        "    #Calculating number of correctly classified samples.\n",
        "    correct = np.array(np.where(softmax_predictions_ == shuffled_labels))\n",
        "    correct = correct.size\n",
        "    print(\"Correct predictions/\", str(percent * 50000/100), ' : ', correct)\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "save_model_path = \"model/\"\n",
        "save_path = saver.save(sess=sess, save_path=save_model_path+\"model.ckpt\")\n",
        "print(\"Model saved in : \", save_path)\n",
        "\n",
        "#Closing the session\n",
        "sess.close()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on :  data_batch_1\n",
            "Working on :  data_batch_2\n",
            "Working on :  data_batch_3\n",
            "Working on :  data_batch_4\n",
            "Working on :  data_batch_5\n",
            "Size of data :  (50000, 32, 32, 3)\n",
            "Size of conv filters bank :  (5, 5, 3, 4)\n",
            "Size of conv result :  (?, 28, 28, 4)\n",
            "Size of relu1 result :  (?, 28, 28, 4)\n",
            "Size of maxpool1 result :  (?, 27, 27, 4)\n",
            "Size of conv filters bank :  (7, 7, 4, 3)\n",
            "Size of conv result :  (?, 21, 21, 3)\n",
            "Size of relu2 result :  (?, 21, 21, 3)\n",
            "Size of maxpool2 result :  (?, 20, 20, 3)\n",
            "Size of conv filters bank :  (5, 5, 3, 2)\n",
            "Size of conv result :  (?, 16, 16, 2)\n",
            "Size of relu3 result :  (?, 16, 16, 2)\n",
            "Size of maxpool3 result :  (?, 15, 15, 2)\n",
            "Fully connected layer results :  Tensor(\"MatMul_7:0\", shape=(?, 10), dtype=float32)\n",
            "Patch :  0\n",
            "Correct predictions/ 40000.0  :  4024\n",
            "Patch :  1\n",
            "Correct predictions/ 40000.0  :  4042\n",
            "Patch :  2\n",
            "Correct predictions/ 40000.0  :  4026\n",
            "Patch :  3\n",
            "Correct predictions/ 40000.0  :  4016\n",
            "Patch :  4\n",
            "Correct predictions/ 40000.0  :  4008\n",
            "Model saved in :  model/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPOxfwvyw_lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset_images(test_patch_path, im_dim=32, num_channels=3):\n",
        "\n",
        "    \"\"\"\n",
        "    Similar to the one used in training except that there is just a single testing binary file for testing the CIFAR10 trained models.\n",
        "    \"\"\"\n",
        "    print(\"Working on testing patch\")\n",
        "    data_dict = unpickle_patch(test_patch_path)\n",
        "    images_data = data_dict[b\"data\"]\n",
        "    dataset_array = np.reshape(images_data, newshape=(len(images_data), im_dim, im_dim, num_channels))\n",
        "    return dataset_array, data_dict[b\"labels\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hiWnqO_4qr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cd28b62d-13ec-4dd9-fda2-b5bb24a34b36"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t     cifar-10-python.tar.gz  log      model\n",
            "cifar-10-batches-py  CNN_train.ipynb\t     main.py  Untitled.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK9IZ4Ku4dBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "19385219-670d-4f6f-a17a-4cf9bc7c0f6d"
      },
      "source": [
        "#Dataset path containing the testing binary file to be decoded.\n",
        "dataset_array, dataset_labels = get_dataset_images(test_patch_path=patches_dir + \"test_batch\", im_dim=32, num_channels=3)\n",
        "print(\"Size of data : \", dataset_array.shape)\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "#Restoring the previously saved trained model.\n",
        "saved_model_path = 'model/'\n",
        "saver = tf.train.import_meta_graph(saved_model_path+'model.ckpt.meta')\n",
        "saver.restore(sess=sess, save_path=saved_model_path+'model.ckpt')\n",
        "\n",
        "#Initalizing the varaibales.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "\"\"\"\n",
        "Restoring previous created tensors in the training phase based on their given tensor names in the training phase.\n",
        "Some of such tensors will be assigned the testing input data and their outcomes (data_tensor, label_tensor, and keep_prop).\n",
        "Others are helpful in assessing the model prediction accuracy (softmax_propabilities and softmax_predictions).\n",
        "\"\"\"\n",
        "softmax_propabilities = graph.get_tensor_by_name(name=\"softmax_probs:0\")\n",
        "softmax_predictions = tf.argmax(softmax_propabilities, axis=1)\n",
        "data_tensor = graph.get_tensor_by_name(name=\"data_tensor:0\")\n",
        "label_tensor = graph.get_tensor_by_name(name=\"label_tensor:0\")\n",
        "keep_prop = graph.get_tensor_by_name(name=\"keep_prop:0\")\n",
        "\n",
        "#keep_prop is equal to 1 because there is no more interest to remove neurons in the testing phase.\n",
        "feed_dict_testing = {data_tensor: dataset_array,\n",
        "                     label_tensor: dataset_labels,\n",
        "                     keep_prop: 1.0}\n",
        "#Running the session to predict the outcomes of the testing samples.\n",
        "softmax_propabilities_, softmax_predictions_ = sess.run([softmax_propabilities, softmax_predictions],\n",
        "                                                      feed_dict=feed_dict_testing)\n",
        "#Assessing the model accuracy by counting number of correctly classified samples.\n",
        "correct = np.array(np.where(softmax_predictions_ == dataset_labels))\n",
        "correct = correct.size\n",
        "print(\"Correct predictions/10,000 : \", correct)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on testing patch\n",
            "Size of data :  (10000, 32, 32, 3)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
            "Correct predictions/10,000 :  1003\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}